

# License & Usage Notice

This repository is published under the Apache License 2.0, strictly for technical review, portfolio purposes, and demonstration of engineering capability.

Commercial use, resale, redistribution, or integration of this code into a production system is not permitted unless explicitly agreed in writing.

Parts of this project were originally designed for a client-specific environment.
All sensitive data, credentials, and proprietary logic have been removed or replaced with neutral examples.
If you wish to use any component of this code in a commercial or client project, please contact me directly to discuss licensing and permissions.

Note:
This repository contains a generalized and cleaned version of a production scraping/data extraction
system originally developed for a commercial environment. All sensitive data, client-specific details,
and private logic have been removed. The project is presented strictly as a technical reference.


## Design Notes & Project Constraints

This system was designed and deployed under strict technical and operational constraints defined by the client.  
The solution had to run reliably on an **Alpine Linux server** with limited resources, without introducing external dependencies or modifying the existing environment.  
All architectural decisions were made to ensure stability, isolation, and predictable execution.

### ğŸ”’ 1. Fully Self-Contained Execution (Alpine Linux)
The platform was required to operate end-to-end using only the resources available on a lightweight **Alpine Linux** server.  
To satisfy this requirement, the system was engineered to:

- run without external cloud services or remote APIs,  
- maintain compatibility with Alpineâ€™s minimal system libraries (musl libc),  
- avoid heavy dependencies that conflict with lightweight Linux distributions,  
- execute predictably through Cron-based orchestration and isolated runtime processes.

This approach ensures high reliability on minimal infrastructure, while preserving full control over the automation workflow.

### ğŸ‘ï¸ 2. Transparent and Supervised Automation Behavior
The automation logic was required to mimic human interaction and remain fully observable during execution.  
As a result, the system:

- performs real-time browser actions,  
- avoids aggressive scraping techniques,  
- uses controlled scrolling, filtering, and page state monitoring,  
- logs every operational step for full traceability.  

This guarantees safe behavior aligned with platform policies and prevents disruptive automated patterns.

### ğŸ’° 3. Architecture Optimized for Budget and Maintainability
The client required a production-oriented tool while avoiding DevOps overhead such as Docker, Kubernetes, cloud databases, or managed services.  
Therefore, the platform was designed as a **zero-infrastructure**, single-folder application that delivers:

- automated ETL workflow (Extract â†’ Transform â†’ Load),  
- configurable YAML-driven behavior,  
- a real-time dashboard served locally,  
- automated proxy validation and data processing pipelines,  
- SSH-based synchronization between Alpine and cPanel environments.

This achieves enterprise-level functionality within the constraints of a cost-efficient deployment model.

---

This design reflects a balance between technical rigor, robustness, client infrastructure limitations, and long-term maintainability.  
The system remains lightweight, transparent, and fully self-contained while supporting complex automation and data processing workflows.


Architecture Summary

The orchestrator container provides the full execution environment for the entire extraction system (dependencies, Python runtime, scheduler, network access, database connectivity).
However, it does not contain any scraper logic inside the image.

Instead, the orchestrator executes the scraper scripts directly from the local repository files.
This design ensures that:
updating or fixing any scraper requires only editing the Python files,
no Docker rebuild is necessary after code changes,
deployment and maintenance remain fast and predictable,
development and production always run on the same codebase.
In short, the container supplies the environment, while the actual logic stays fully editable outside the image.


Main Components
Automated scraping modules (scheduled, containerized)
Python orchestration layer (runner.py)
PostgreSQL main database + Metabase internal DB
Metabase dashboards (pricing insights)
Traefik reverse proxy with SSL and Basic Auth
Full Dev Mode version (non-Docker) for development & debugging
Full project documentation inside DOCUMENTATION/


Demo Videos

This repository includes a set of demonstration videos created during the development process.
They were originally prepared for client communication and milestone validation.
These videos provide high-level functional overviews and visual context, but they do not replace the official technical documentation.

## Demo Videos
- **Milestone 1:**  
  https://www.youtube.com/watch?v=ijqTALUnk1k  
- **Milestone 2:**  
  https://www.youtube.com/watch?v=iBuDspQvl5s  
- **Milestone 3:**  
  https://www.youtube.com/watch?v=RJLspqNEm1U  
- **Milestone 4:**  
  https://www.youtube.com/watch?v=_Zp7e7ik51w  
## Portfolio Reference 
- https://www.upwork.com/freelancers/~01207dc9df982f92c4?p=1955092061314666496



## 1. Clone the Repository

```bash
git clone https://github.com/PhoenixZuko/pricing-intelligence-platform
cd pricing-intelligence-platform

 PROJECT: PRICING DATA PLATFORM
 Author: Andrei Sorin È˜tefan
```bash

Pricing-Project/
â”œâ”€â”€ docker-compose.yml          # Main Docker Compose file: defines all services (DBs, API, Orchestrator, Scheduler, Metabase, Traefik)
â”œâ”€â”€ requirements.txt            # Common Python dependencies
â”œâ”€â”€ runner.py                   # Entry point for Orchestrator; executes scrapers
â”‚
â”œâ”€â”€ DOCUMENTATION/              # Documentation and guides
â”‚   â”œâ”€â”€ ARCHITECTURE.md         # Detailed architecture explanation
â”‚   â”œâ”€â”€ Change_scraper_schedule.md # How to change the cron schedule for scrapers
â”‚   â”œâ”€â”€ DEV_MODE.md             # Instructions for running project in Dev Mode (no Docker)
â”‚   â”œâ”€â”€ INSTALLATION.md         # Installation instructions
â”‚   â”œâ”€â”€ INSTALL_Guide.md        # Additional setup guide
â”‚   â”œâ”€â”€ PricingScraper-DevMode.zip # Full Dev Mode package (non-dockerized version)
â”‚   â”œâ”€â”€ README.md               # Documentation overview
â”‚   â”œâ”€â”€ USAGE.md                # How to use the system
â”‚   â””â”€â”€ VIDEO_GUIDE.md          # Script/guide for client presentation video
â”‚
â”œâ”€â”€ Scrapers/                   # Data extraction scripts (scrapers)
â”‚   â”œâ”€â”€ 1cdz-scraper.py         # Individual scraper for source 1 (example: cdz)
â”‚   â”œâ”€â”€ 2dino-scraper.py        # Scraper for source 2
â”‚   â”œâ”€â”€ 3klebs-scraper.py       # Scraper for source 3
â”‚   â”œâ”€â”€ 4main_clearago.py       # Scraper for source 4 (Clearago)
â”‚   â”œâ”€â”€ 5main_entsorgo.py       # Scraper for source 5 (Entsorgo)
â”‚   â”œâ”€â”€ core_clearago/          # Core scraping logic specific to Clearago
â”‚   â”œâ”€â”€ core_ensorgo/           # Core scraping logic specific to Entsorgo
â”‚   â”œâ”€â”€ cvs_maker.py            # Utility to export results to CSV
â”‚   â”œâ”€â”€ json_maker.py           # Utility to export results to JSON
â”‚   â””â”€â”€ README.md               # Notes and usage for scrapers
â”‚
â”œâ”€â”€ archive/                    # Archived results of past scraper runs
â”‚   â”œâ”€â”€ results_data_07_10_2025 # Example archived dataset
â”‚   â””â”€â”€ results_data_08_10_2025 # Example archived dataset
â”‚
â”œâ”€â”€ category_parser/            # Category parsing module
â”‚   â”œâ”€â”€ create_category.py      # Script to create category mappings
â”‚   â”œâ”€â”€ create_type.py          # Script to create type mappings
â”‚   â”œâ”€â”€ output_data_with_type.csv # Example output file with type/category
â”‚   â”œâ”€â”€ type_definitions/       # Folder containing category/type definitions
â”‚   â””â”€â”€ README.md               # Notes for category parser usage
â”‚
â”œâ”€â”€ config.yaml                 # Global configuration file
â”‚
â”œâ”€â”€ database/                   # Database initialization scripts
â”‚   â”œâ”€â”€ db_setup.py             # Script to set up database schema/tables
â”‚   â””â”€â”€ README.md               # Documentation for DB setup
â”‚
â”œâ”€â”€ flask_api/                  # Flask REST API service
â”‚   â”œâ”€â”€ Dockerfile              # Docker build file for API
â”‚   â”œâ”€â”€ app.py                  # Main Flask app (defines endpoints)
â”‚   â”œâ”€â”€ auth.py                 # Authentication logic (BasicAuth)
â”‚   â”œâ”€â”€ db.py                   # Database connection/queries
â”‚   â”œâ”€â”€ requirements.txt        # API-specific dependencies
â”‚   â”œâ”€â”€ README.md               # API usage notes
â”‚   â””â”€â”€ __pycache__/            # Compiled Python cache (auto-generated)
â”‚
â”œâ”€â”€ metabase/                   # Metabase container (dashboards/visualizations)
â”œâ”€â”€ metabase-postgres/          # PostgreSQL DB for Metabase configuration (users, dashboards)
â”œâ”€â”€ postgres/                   # PostgreSQL DB for scraper data
â”‚   â””â”€â”€ Dockerfile              # Docker build file for Postgres
â”‚
â”œâ”€â”€ orchestrator/               # Orchestrator service (controls scraper runs)
â”‚   â”œâ”€â”€ Dockerfile              # Docker build file for orchestrator
â”‚   â””â”€â”€ requirements.txt        # Orchestrator-specific dependencies
â”‚
â”œâ”€â”€ scheduler/                  # Scheduler service (cron jobs)
â”‚   â”œâ”€â”€ Dockerfile              # Docker build file for scheduler
â”‚   â””â”€â”€ entrypoint.sh           # Entrypoint script to run cron jobs
â”‚
â”œâ”€â”€ traefik/                    # Traefik reverse proxy (routing & SSL)
â”‚   â”œâ”€â”€ traefik.yml             # Main Traefik config
â”‚   â”œâ”€â”€ traefik_dynamic.yml     # Dynamic routing rules for API/Metabase
â”‚   â”œâ”€â”€ acme.json               # Stores SSL certificates (Let's Encrypt)
â”‚
â””â”€â”€ utils/                      # Utility scripts
    â”œâ”€â”€ cleaner.py              # Script for cleaning/preprocessing data
    â””â”€â”€ README.md               # Notes for utilities



Dockerized services (flask_api, orchestrator, scheduler, postgres, metabase, traefik).
Core logic (scrapers, orchestrator, API).
Docs + configs.


Workflow ASCII diagram
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚        Scheduler         â”‚
                         â”‚ - Runs cron jobs         â”‚
                         â”‚ - Triggers Orchestrator  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚       Orchestrator       â”‚
                         â”‚ - Executes scrapers      â”‚
                         â”‚ - Installs dependencies  â”‚
                         â”‚ - Uses local scraper codeâ”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                       writes data   â”‚
                                     â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  PostgreSQL (Scrapers DB) â”‚
                       â”‚ - Stores extracted data   â”‚
                       â”‚ - Persistent volume       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                                                       â”‚
      â–¼                                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        API          â”‚                          â”‚        Metabase          â”‚
â”‚ - Flask REST API    â”‚                          â”‚ - Dashboards & Charts    â”‚
â”‚ - Secure via Traefikâ”‚                          â”‚ - Uses its own Postgres  â”‚
â”‚ - Serves data JSON  â”‚                          â”‚ - Reads Scrapers DB      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                              â”‚
            â”‚ external access                              â”‚ external access
            â–¼                                              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚         Traefik          â”‚                 â”‚         Traefik          â”‚
   â”‚ - Reverse proxy          â”‚                 â”‚ - SSL certificates       â”‚
   â”‚ - Routes /api and /      â”‚                 â”‚ - Secure HTTPS access    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. Configure Environment

After cloning, you must configure the environment variables:

a. Copy the example file:

     cp .env.example .env

b. Open .env and adjust the following values:
Database Settings
   Set usernames and passwords for both:
   Scrapers database (preismatrix_data)
   Metabase internal database (metabase_data)

Scheduler
   Configure the scraping frequency:
      SCRAPER_SCHEDULE=7 â†’ run every 7 days
      SCRAPER_SCHEDULE=10 â†’ run every 10 days
      SCRAPER_SCHEDULE=30 â†’ run every 30 days

Traefik / SSL / Domain
Set your domain and contact email for SSL via Letâ€™s Encrypt.

3. Configure Traefik Basic Auth (Access Password)
   Generate a new password hash (replace NewPassword with your password):

  Run:
   docker run --rm httpd:2.4 htpasswd -nbB admin NewPassword
   Copy the generated line (e.g. admin:$2y$05$...).

 Open:
   traefik/traefik_dynamic.yml

  Replace the existing line with your new hash:
     - "admin:$2y$05$R2.X3SxsS4n3yBf/ZsFaFuMVdRArspbY8nuqUsfs2lBA5LKQzY3.O"

   Restart Traefik:
     docker compose restart traefik

Note: Do not store the password hash in .env, because Docker Compose interprets $ characters.
 ### ğŸ”’ Important Security Note
       Based on experience and testing, we found that the `$` symbol inside passwords is interpreted by Docker Compose as the  beginning of an environment variable.  
       This behavior can cause issues when reading passwords and may break the system.  
       To avoid such problems and as an additional security measure, we decided **not to store password hashes inside `.    env`**.  
       Instead, they are managed directly in `traefik_dynamic.yml`.

4. Start the System

Once everything is configured, start the platform with:

     docker compose up -d


 5. Access the Services
     Platform Domain: https://your-domain.com
     Metabase UI: https://your-domain.com/metabase
     API: https://your-domain.com/api-matrix

Summary
Copy .env.example â†’ .env
Set database credentials
Configure scheduler
Adjust Traefik (domain, SSL, Basic Auth)
Start with docker compose up -d
You are now ready to use the Pricing Data Platform

---




## 6. Documentation

For more details, please check the `DOCUMENTATION` folder in this repository.  
It contains additional guides, usage notes, and explanations.  

You will also find a file named `PricingScraper-DevMode.zip`.  
This archive includes the **full documentation for running all scripts without Docker**, including:
- Full dependency list
- Installation steps
- Script execution order
- Complete data flow description

---


## 7. Development Notes

- **Modifications & New Functions**  
  Any new functionality or configuration changes should be made directly in:


runner.py
This file is the main entry point that orchestrates all other scripts.
- **Direct Script Access**  
Even though the system is dockerized, all programs are accessible in their respective folders.  
`runner.py` reads them directly from the repository structure, which makes development and debugging straightforward.

---

## Next Steps

1. For **basic setup and first run**, follow the steps in this README.  
2. For **advanced configurations and development**, consult:
 - The `DOCUMENTATION` folder  
 - `PricingScraper-DevMode.zip` for the complete standalone (non-Docker) flow
