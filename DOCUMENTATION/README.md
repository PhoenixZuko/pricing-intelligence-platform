 PROJECT: PRICING DATA PLATFORM
 Author: Andrei Sorin È˜tefan
```bash

Pricing-Project/
â”œâ”€â”€ docker-compose.yml          # Main Docker Compose file: defines all services (DBs, API, Orchestrator, Scheduler, Metabase, Traefik)
â”œâ”€â”€ requirements.txt            # Common Python dependencies
â”œâ”€â”€ runner.py                   # Entry point for Orchestrator; executes scrapers
â”‚
â”œâ”€â”€ DOCUMENTATION/              # Documentation and guides
â”‚   â”œâ”€â”€ ARCHITECTURE.md         # Detailed architecture explanation
â”‚   â”œâ”€â”€ Change_scraper_schedule.md # How to change the cron schedule for scrapers
â”‚   â”œâ”€â”€ DEV_MODE.md             # Instructions for running project in Dev Mode (no Docker)
â”‚   â”œâ”€â”€ INSTALLATION.md         # Installation instructions
â”‚   â”œâ”€â”€ INSTALL_Guide.md        # Additional setup guide
â”‚   â”œâ”€â”€ PricingScraper-DevMode.zip # Full Dev Mode package (non-dockerized version)
â”‚   â”œâ”€â”€ README.md               # Documentation overview
â”‚   â”œâ”€â”€ USAGE.md                # How to use the system
â”‚   â””â”€â”€ VIDEO_GUIDE.md          # Script/guide for client presentation video
â”‚
â”œâ”€â”€ Scrapers/                   # Data extraction scripts (scrapers)
â”‚   â”œâ”€â”€ 1cdz-scraper.py         # Individual scraper for source 1 (example: cdz)
â”‚   â”œâ”€â”€ 2dino-scraper.py        # Scraper for source 2
â”‚   â”œâ”€â”€ 3klebs-scraper.py       # Scraper for source 3
â”‚   â”œâ”€â”€ 4main_clearago.py       # Scraper for source 4 (Clearago)
â”‚   â”œâ”€â”€ 5main_entsorgo.py       # Scraper for source 5 (Entsorgo)
â”‚   â”œâ”€â”€ core_clearago/          # Core scraping logic specific to Clearago
â”‚   â”œâ”€â”€ core_ensorgo/           # Core scraping logic specific to Entsorgo
â”‚   â”œâ”€â”€ cvs_maker.py            # Utility to export results to CSV
â”‚   â”œâ”€â”€ json_maker.py           # Utility to export results to JSON
â”‚   â””â”€â”€ README.md               # Notes and usage for scrapers
â”‚
â”œâ”€â”€ archive/                    # Archived results of past scraper runs
â”‚   â”œâ”€â”€ results_data_07_10_2025 # Example archived dataset
â”‚   â””â”€â”€ results_data_08_10_2025 # Example archived dataset
â”‚
â”œâ”€â”€ category_parser/            # Category parsing module
â”‚   â”œâ”€â”€ create_category.py      # Script to create category mappings
â”‚   â”œâ”€â”€ create_type.py          # Script to create type mappings
â”‚   â”œâ”€â”€ output_data_with_type.csv # Example output file with type/category
â”‚   â”œâ”€â”€ type_definitions/       # Folder containing category/type definitions
â”‚   â””â”€â”€ README.md               # Notes for category parser usage
â”‚
â”œâ”€â”€ config.yaml                 # Global configuration file
â”‚
â”œâ”€â”€ database/                   # Database initialization scripts
â”‚   â”œâ”€â”€ db_setup.py             # Script to set up database schema/tables
â”‚   â””â”€â”€ README.md               # Documentation for DB setup
â”‚
â”œâ”€â”€ flask_api/                  # Flask REST API service
â”‚   â”œâ”€â”€ Dockerfile              # Docker build file for API
â”‚   â”œâ”€â”€ app.py                  # Main Flask app (defines endpoints)
â”‚   â”œâ”€â”€ auth.py                 # Authentication logic (BasicAuth)
â”‚   â”œâ”€â”€ db.py                   # Database connection/queries
â”‚   â”œâ”€â”€ requirements.txt        # API-specific dependencies
â”‚   â”œâ”€â”€ README.md               # API usage notes
â”‚   â””â”€â”€ __pycache__/            # Compiled Python cache (auto-generated)
â”‚
â”œâ”€â”€ metabase/                   # Metabase container (dashboards/visualizations)
â”œâ”€â”€ metabase-postgres/          # PostgreSQL DB for Metabase configuration (users, dashboards)
â”œâ”€â”€ postgres/                   # PostgreSQL DB for scraper data
â”‚   â””â”€â”€ Dockerfile              # Docker build file for Postgres
â”‚
â”œâ”€â”€ orchestrator/               # Orchestrator service (controls scraper runs)
â”‚   â”œâ”€â”€ Dockerfile              # Docker build file for orchestrator
â”‚   â””â”€â”€ requirements.txt        # Orchestrator-specific dependencies
â”‚
â”œâ”€â”€ scheduler/                  # Scheduler service (cron jobs)
â”‚   â”œâ”€â”€ Dockerfile              # Docker build file for scheduler
â”‚   â””â”€â”€ entrypoint.sh           # Entrypoint script to run cron jobs
â”‚
â”œâ”€â”€ traefik/                    # Traefik reverse proxy (routing & SSL)
â”‚   â”œâ”€â”€ traefik.yml             # Main Traefik config
â”‚   â”œâ”€â”€ traefik_dynamic.yml     # Dynamic routing rules for API/Metabase
â”‚   â”œâ”€â”€ acme.json               # Stores SSL certificates (Let's Encrypt)
â”‚
â””â”€â”€ utils/                      # Utility scripts
    â”œâ”€â”€ cleaner.py              # Script for cleaning/preprocessing data
    â””â”€â”€ README.md               # Notes for utilities



Dockerized services (flask_api, orchestrator, scheduler, postgres, metabase, traefik).
Core logic (scrapers, orchestrator, API).
Docs + configs.


Workflow ASCII diagram
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚        Scheduler         â”‚
                         â”‚ - Runs cron jobs         â”‚
                         â”‚ - Triggers Orchestrator  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚       Orchestrator       â”‚
                         â”‚ - Executes scrapers      â”‚
                         â”‚ - Installs dependencies  â”‚
                         â”‚ - Uses local scraper codeâ”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                       writes data   â”‚
                                     â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  PostgreSQL (Scrapers DB) â”‚
                       â”‚ - Stores extracted data   â”‚
                       â”‚ - Persistent volume       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                                                       â”‚
      â–¼                                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        API          â”‚                          â”‚        Metabase          â”‚
â”‚ - Flask REST API    â”‚                          â”‚ - Dashboards & Charts    â”‚
â”‚ - Secure via Traefikâ”‚                          â”‚ - Uses its own Postgres  â”‚
â”‚ - Serves data JSON  â”‚                          â”‚ - Reads Scrapers DB      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                              â”‚
            â”‚ external access                              â”‚ external access
            â–¼                                              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚         Traefik          â”‚                 â”‚         Traefik          â”‚
   â”‚ - Reverse proxy          â”‚                 â”‚ - SSL certificates       â”‚
   â”‚ - Routes /api and /      â”‚                 â”‚ - Secure HTTPS access    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Note: Orchestrator Design
The Orchestrator runs inside a Docker container and comes with all scraper dependencies pre-installed.
However, it executes runner.py, which is placed at the root of the project.
Even though it runs dockerized, the Orchestrator directly reads the local scraper files mounted into the container.

âœ… This means:

Scrapers can be modified instantly (no need to rebuild Docker images).
New extractors or new functions can be added easily by just dropping new files.
The system remains production-ready, but still very developer-friendly.
ğŸ”‘ This makes the Orchestrator a powerful and well-designed engine: flexible, easy to extend, and easy to maintain.


# ğŸ“– Pricing Platform â€“ Documentation



Welcome! This folder contains all the documentation needed to **install, configure, and use** the Pricing Platform.  

Use this README as an index and quick start guide.



---



## ğŸ“‚ Documentation Contents



- **INSTALLATION.md** â†’ Full step-by-step installation guide (fresh install or safe restart).  
- **INSTALL_Guide.md** â†’ Quick setup guide for developers.  
- **USAGE.md** â†’ How to use the system after installation (common commands, daily operations).  
- **Change_scraper_schedule.md** â†’ How to change how often scrapers run (scheduler configuration).  
- **ARCHITECTURE.md** â†’ System architecture and technical overview.  
- **VIDEO_GUIDE.md** â†’ (optional) Placeholder for a video tutorial.  
- **README.md** â†’ This file, overview of documentation.  



---



## ğŸš€ Quick Start



```bash

# 1. Requirements

# - Docker >= 20.10

# - Docker Compose >= 2.5

# - Linux server (Ubuntu/Debian recommended)



# 2. Setup Environment

# Copy `.env.example` â†’ rename to `.env`

# Configure databases, domain, schedule inside `.env`



# 3. Start / Stop Services

## Start all services (in background)
docker compose up -d

## Stop all services (keep data)
docker compose down

## Stop and remove everything (âš ï¸ deletes databases!)
docker compose down -v




# 4. Access the Platform

https://<your-domain>



---





 âš™ï¸ Daily Operations



# Restart all services
docker compose restart



# Check logs (all services)

docker compose logs -f



# Enter scheduler container

docker compose exec scheduler bash



For a full list of useful commands, see USAGE.md.





ğŸ”’ Security



# Generate a new Traefik password hash
docker run --rm httpd:2.4 htpasswd -nb admin NewStrongPassword



# Edit traefik/traefik_dynamic.yml and update the hash

# Then restart Traefik
docker compose restart traefik





ğŸ“… Scheduler



# Default: runs every 7 days at 2 AM

# To change frequency: edit `.env`

SCRAPER_SCHEDULE=<days>



# After updating restart the scheduler

docker compose restart scheduler



# Verify the new schedule

docker compose exec scheduler crontab -l



ğŸ’¾ Backups



# List volumes

docker volume ls



# Backup main PostgreSQL database

docker run --rm -v pricing_pgdata:/data -v $(pwd):/backup busybox tar czf /backup/pgdata.tar.gz /data





Architecture

The system includes:
Scrapers (data extraction)
Orchestrator & Scheduler (automation)
PostgreSQL databases (scrapers + Metabase)
Metabase (dashboards and analytics)
Traefik (reverse proxy & SSL)

For detailed diagrams and explanations, see ARCHITECTURE.md.
